{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "“7_1_SequenceModelling.ipynb”的副本",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45175319f6d94fa9b536c57a7a7ac201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e150bcd371fe4247b6fa98f38d278e2d",
              "IPY_MODEL_02b34afa1ce244aa933d366ee06ed036"
            ],
            "layout": "IPY_MODEL_8b6bfddbcbff4fb3b23d1d8dce8dc01f"
          }
        },
        "e150bcd371fe4247b6fa98f38d278e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1821e68eeb194271a7135f7cfcc91242",
            "max": 600901,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f549e149132e4db19e0fbdb6df0104f8",
            "value": 600901
          }
        },
        "02b34afa1ce244aa933d366ee06ed036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cd6cc1eef354b3bbb6554c1989f70f1",
            "placeholder": "​",
            "style": "IPY_MODEL_43b74c0b719a4bfca209f195fa951f93",
            "value": " 601088/? [00:10&lt;00:00, 54732.85it/s]"
          }
        },
        "8b6bfddbcbff4fb3b23d1d8dce8dc01f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1821e68eeb194271a7135f7cfcc91242": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f549e149132e4db19e0fbdb6df0104f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "8cd6cc1eef354b3bbb6554c1989f70f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43b74c0b719a4bfca209f195fa951f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuanRunwei/COMP6248-Deep-Learning/blob/labs/%E2%80%9C7_1_SequenceModelling_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI7_vVXnKBjO"
      },
      "source": [
        "# Part 1: Sequence Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvSIeMVRKBjd"
      },
      "source": [
        "__Before starting, we recommend you enable GPU acceleration if you're running on Colab.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgGJ6gTSKBjd",
        "outputId": "a8ed492f-c6e2-45a3-9ecc-790d5cd76ab6"
      },
      "source": [
        "# Execute this code block to install dependencies when running on colab\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "try: \n",
        "    import torchbearer\n",
        "except:\n",
        "    !pip install torchbearer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchbearer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e9/4049a47dd2e5b6346a2c5d215b0c67dce814afbab1cd54ce024533c4834e/torchbearer-0.5.3-py3-none-any.whl (138kB)\n",
            "\r\u001b[K     |██▍                             | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20kB 26.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30kB 21.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 51kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 61kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 71kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 92kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 102kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 112kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 122kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 133kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchbearer) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from torchbearer) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchbearer) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->torchbearer) (3.7.4.3)\n",
            "Installing collected packages: torchbearer\n",
            "Successfully installed torchbearer-0.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewbmFjN1KBjf"
      },
      "source": [
        "## Markov chains\n",
        "\n",
        "We'll start our exploration of modelling sequences and building generative models using a 1st order Markov chain. The Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In our case we're going to learn a model over a set of characters from an English language text. The events, or states, in our model are the set of possible characters, and we'll learn the probability of moving from one character to the next.\n",
        "\n",
        "Let's start by loading the data from the web:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "45175319f6d94fa9b536c57a7a7ac201",
            "e150bcd371fe4247b6fa98f38d278e2d",
            "02b34afa1ce244aa933d366ee06ed036",
            "8b6bfddbcbff4fb3b23d1d8dce8dc01f",
            "1821e68eeb194271a7135f7cfcc91242",
            "f549e149132e4db19e0fbdb6df0104f8",
            "8cd6cc1eef354b3bbb6554c1989f70f1",
            "43b74c0b719a4bfca209f195fa951f93"
          ]
        },
        "id": "BI7krHggKBjf",
        "outputId": "5c906801-1b11-4f3d-c031-711994cb4778"
      },
      "source": [
        "from torchvision.datasets.utils import download_url\n",
        "import torch\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "# Read the data\n",
        "download_url('https://s3.amazonaws.com/text-datasets/nietzsche.txt', '.', 'nietzsche.txt', None)\n",
        "text = io.open('./nietzsche.txt', encoding='utf-8').read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://s3.amazonaws.com/text-datasets/nietzsche.txt to ./nietzsche.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45175319f6d94fa9b536c57a7a7ac201",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=600901.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "corpus length: 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiA8JpkLKBjh"
      },
      "source": [
        "We now need to iterate over the characters in the text and count the times each transition happens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrZhfXeLKBjh"
      },
      "source": [
        "transition_counts = dict()\n",
        "for i in range(0,len(text)-1):\n",
        "    currc = text[i]\n",
        "    nextc = text[i+1]\n",
        "    if currc not in transition_counts:\n",
        "        transition_counts[currc] = dict()\n",
        "    if nextc not in transition_counts[currc]:\n",
        "        transition_counts[currc][nextc] = 0\n",
        "    transition_counts[currc][nextc] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB7TbLbMKBji"
      },
      "source": [
        "The `transition_counts` dictionary maps the current character to the next character, and this is then mapped to a count. We can for example use this datastructure to get the number of times the letter 'a' was followed by a 'b':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KBv75hKKBji",
        "outputId": "ff4353e3-3bdb-4c63-ef14-451101dd218c"
      },
      "source": [
        "print(\"Number of transitions from 'a' to 'b': \" + str(transition_counts['a']['b']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of transitions from 'a' to 'b': 813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-zICcPxKBji"
      },
      "source": [
        "Finally, to complete the model we need to normalise the counts for each initial character into a probability distribution over the possible next character. We'll slightly modify the form we're storing these and maintain a tuple of array objects for each initial character: the first holding the set of possible characters, and the second holding the corresponding probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG_uUUvEKBjj"
      },
      "source": [
        "transition_probabilities = dict()\n",
        "for currentc, next_counts in transition_counts.items():\n",
        "  values, probabilities = [], []\n",
        "  sumall = 0\n",
        "  for nextc, count in next_counts.items():\n",
        "    values.append(nextc)\n",
        "    probabilities.append(count)\n",
        "    sumall += count\n",
        "  for i in range(len(probabilities)):\n",
        "    probabilities[i] /= sumall\n",
        "  transition_probabilities[currentc] = (values, probabilities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSPaYK_XKBjk"
      },
      "source": [
        "At this point, we could print out the probability distribution for a given initial character state. For example, to print the distribution for 'a':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJge1hxMKBjk",
        "outputId": "f3f97da2-ef25-4725-edee-7d52d7804484"
      },
      "source": [
        "for a,b in zip(transition_probabilities['a'][0], transition_probabilities['a'][1]):\n",
        "    print(a,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c 0.03685183172083922\n",
            "t 0.14721708881400153\n",
            "  0.05296771388194369\n",
            "n 0.2322806826829003\n",
            "l 0.11552886183280792\n",
            "r 0.08794434177628004\n",
            "s 0.0968583541689314\n",
            "v 0.0192412218719426\n",
            "i 0.03402543754755952\n",
            "d 0.026986628981411024\n",
            "g 0.017202956843135123\n",
            "y 0.02505707142080661\n",
            "k 0.012827481247961734\n",
            "b 0.02209479291227307\n",
            "p 0.020545711490379388\n",
            "m 0.02030111968692249\n",
            "u 0.011414284161321883\n",
            "f 0.004429829329274921\n",
            "w 0.004837482335036417\n",
            ", 0.0010870746820306554\n",
            "\n",
            " 0.005353842809000978\n",
            "z 0.0006522448092183933\n",
            "x 0.0007609522774214588\n",
            "o 0.0005435373410153277\n",
            ". 0.000489183606913795\n",
            "- 0.0004348298728122622\n",
            "' 5.4353734101532776e-05\n",
            "j 0.0004348298728122622\n",
            "h 0.00035329927165996303\n",
            "e 0.0007337754103706925\n",
            ": 5.4353734101532776e-05\n",
            "a 5.4353734101532776e-05\n",
            ") 0.00010870746820306555\n",
            "! 2.7176867050766388e-05\n",
            "; 2.7176867050766388e-05\n",
            "\" 8.153060115229916e-05\n",
            "q 2.7176867050766388e-05\n",
            "_ 8.153060115229916e-05\n",
            "[ 2.7176867050766388e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clgbn4sWKBjk"
      },
      "source": [
        "It looks like the most probable letter to follow an 'a' is 'n'. \n",
        "\n",
        "__What is the most likely letter to follow the letter 'j'? Write your answer in the block below:__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdCX_kimKBjk",
        "outputId": "3ae92f8d-10f6-4d60-e0ec-2fee8f1a8851"
      },
      "source": [
        "print(transition_probabilities['j'][0][0], \":\", transition_probabilities['j'][1][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e : 0.2585278276481149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JFS7UzIKBjk"
      },
      "source": [
        "We mentioned earlier that the Markov model is generative. This means that we can draw samples from the distributions and iteratively move between states. \n",
        "\n",
        "Use the following code block to iteratively sample 1000 characters from the model, starting with an initial character 't'. You can use the `torch.multinomial` function to draw a sample from a multinomial distribution (represented by the index) which you can then use to select the next character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4mloUouKBjl",
        "outputId": "efd3b9f5-eb9a-4de4-d808-5c2330fdc831"
      },
      "source": [
        "current = 't'\n",
        "\n",
        "for i in range(0, 1000):\n",
        "    print(current, end='')\n",
        "    character_index = torch.multinomial(torch.tensor(transition_probabilities['t'][1]), 1)[0]\n",
        "    character_value = transition_probabilities['t'][0][character_index]\n",
        "    current = character_value\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "taihhhhhhire hioih , s eauhhe hh uhoh ihhhr\n",
            "euhh ehh heuhoohh \n",
            " ohhe . hrohih oiohuihhhiehe hihselu  heh    et:hhhaho,\n",
            " horh  rhh \n",
            "yhhhouoehhh   oie, o ihish ih,ui hehyh,ho h hhhheaiohehhohca oh hhhhhi lhheeh heha  ihha hh leherhharhhh i hheh,h hhhohyhh:iha,ohh e eehsh\n",
            "rr\n",
            "hhhhheui\n",
            "hr eiuhhhsh h h:hhohih y raiu hshiehorhahh hhs a -ha,\n",
            "ss ehhhtit mhhihi ahe ahaea,ohoyhihaoiehhe aohhha uhhrya hihahoohat oineea hhh.ohh-iiywieh n esi,e iooh\n",
            "olh sheeiuuh. hhoheeh a ea!ulhhahisohali etifh  o ooh yooo iou iho rhyh eehhhhh sa hy \n",
            "hoh i ii h orhy he,rh,o hi hhrhhyaouhhheoi hhheish  hiiho u yho o yehoiahohoaer  hrthrshoo hohoe\n",
            "h lsyhu hh  a hal\n",
            "  r\n",
            "hhhhiihihhu oa e,iso,hh\n",
            "h hiihlee eheh hyy oiyh euhoohiho;o s haaeho h\n",
            "\n",
            "hei\n",
            "  s h iouyhhhisoiiehhoohh rhmoohohhsehh   yehheh,ooe\n",
            " iea hn eehyui hhl, h i iyouoh\n",
            "hhoy hhhoe trihihiohha e ootsh  hsth i \n",
            " eohu  iyee ehahoiiieiooasiiwhhi  a iiyi\"hohooh ouohhhrea rhohsh ehihs ha  eh ho  hhaih loha ihye seho hs ,se  i hyeshhiyehhrheh   ,h  iho oei\n",
            " hi  hahhh "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS0jLJehKBjm"
      },
      "source": [
        "You should observe a result that is clearly not English, but it should be obvious that some of the common structures in the English language have been captured.\n",
        "\n",
        "__Rather than building a model based on individual characters, can you implement a model in the following code block that works on words instead?__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9COfC5ZOKBjn",
        "outputId": "eb30399f-644b-4ea3-dfbb-91d352413843"
      },
      "source": [
        "text_clean = text.replace(\"\\n\", \" \").replace(\"-\", \" \").replace(\"--\", \" \").replace(\"?\", \" \").replace(\"!\", \" \").replace(\",\", \" \").replace(\".\", \" \").replace(\":\", \" \").replace(\";\", \" \")\n",
        "word_text = text_clean.strip().split(\" \")\n",
        "word_text = [item for item in word_text if item is not '']\n",
        "\n",
        "word_transition_counts = dict()\n",
        "for i in range(len(word_text)-1):\n",
        "  current_word = word_text[i]\n",
        "  next_word = word_text[i+1]\n",
        "  if current_word not in word_transition_counts:\n",
        "    word_transition_counts[current_word] = dict()\n",
        "  if next_word not in word_transition_counts[current_word]:\n",
        "    word_transition_counts[current_word][next_word] = 0\n",
        "  word_transition_counts[current_word][next_word] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "word_transition_probability = dict()\n",
        "for current, next in word_transition_counts.items():\n",
        "  values, probabilities = [], []\n",
        "  sumall = 0\n",
        "  for nextc, count in next.items():\n",
        "    values.append(nextc)\n",
        "    probabilities.append(count)\n",
        "    sumall += count\n",
        "  for i in range(len(probabilities)):\n",
        "    probabilities[i] /= sumall\n",
        "  word_transition_probability[current] = (values, probabilities)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(0, 1000):\n",
        "    print(current, end=' ')\n",
        "    word_index = torch.multinomial(torch.tensor(word_transition_probability['a'][1]), 1)[0]\n",
        "    word_value = word_transition_probability['a'][0][word_index]\n",
        "    current = word_value\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "indications flexible moralist soul court more thing man world piece cry race power kind sensation manner punishment hundred supposition costly christian higher means hatred question certain repulsive future man realm foundation doubt trick decade presumptive purely synthetic series un sort sense prohibition german brac result decided trick means certain man delusion right powerful hint right \"saint soul pride \"saint fool's little proper vast religious punishment god law height sensible youth master whole shrinking solitary universally unity brac matter complex depraved fearful feeling beautiful labyrinth hard passion requirement right period form philosophy living misuse mask greater period deception match slender form condition true wish certain sort stupid hero victorious thinker backward concealment short long lower hundred good storeroom brac person humanitarian commencement continent plebeian substance philosopher delusion cumbersome god sensible certain typical free learner deterioration new whole great man totally favourable new renewal luther man mortal considerable rococo soul puzzle creator ruling stick physiological fearful special way good theory power puzzle mortal something thoughtful time comparative new long primary series society novelty little strong foreign point step law continent forward german home forward much source punishment very philosopher sympathetic german piece term man gregarious christian personal wrong dangerous child middle labyrinth thinker highly malady superiority remnant second danger comparison still satisfaction penitential trick woman religion sinking deliverance person time christian much few thoughtful frittering god degenerating transvaluation false dangerous corrupter metaphysical particularly sign christian mask metaphysical man's degenerating person law thunder relation people hero dangerous woman prince seductive \"bad half dilettante heart small \"necessary\" sure bad transitory question \"saint fearful little thing noble period bad whole part religion more portentous close pride variety defeminising reader second chemistry view learner bond tragedy woman valid good homesickness far philosopher thing noble great sort theory napoleonic thirst share free new long new moral fool tell youth sort \"why sentiment new world material new remarkable man reader \"common delay hundred spirit creator defect cave place future guise greater long certain good combination whole direct corps life monster philosopher soul fancy court philosopher comedy time moral tincture respectful surplus naively sinking \"free proud degree heart strong far result violation problem consequence sort diversified prejudiced decided great napoleonic socialist problem woman's bad doubt chemistry belief species god moment description future powerful something learned free purely mask general play little problem predominant good bond long still magical promise conclusion delusion man desire kind charm rule world hammer person similar means situation good dogmatist moment far single recluse's step pessimist desire manner good satyric great good third young matter cyclops smile proper means guise philosopher curiosity one personal learned bad spirit hero repulsive defect scientific successful hand sympathy far proper morality blindness higher two reflection kind definite fine large sensitive purely god joy world military statesman knowledge fine predilection being delusion) third gentle mask large lower complete circe man whole desire triumph time long distinguished constraint good beautiful defeminising laborer manner philosophy complete characteristic distinguished rational church parable philosophy new will species detriment hearing pestiferous deficient being commander man short rich piece primordial thoughtful time future costume lower matter day totally delay compensation philosopher metaphysician's result person people well certain potent mask predisposition \"theory supermoral bad basis better sickly profitless doctrine boisterous malady higher carnival higher \"nation\" merrier box step feminine noble costly soul thinker mainstay harem man mode primordial great characteristic bad third gold word sense delay cloak general sovereign mirror veritable preponderance foundation creating pleasure personal twilight complex music conclusion mild labyrinth complex basic different system poet dialectic res man right task hard degree problem moment southerner thing noble thing profound combination complete living confusion moment box poet phenomenon necessary condition doctrine \"will predisposition mausoleum valid reluctance foundation pride flush little tree reason philosopher sort dangerous bad better being lynx single sort person greater un one phantom subsequent feminine step moralist christian comedy step supposition cold categorical synthesis century mystery sort boisterous theology primordial future storm saint condition purely majority still hundred burning folly parable painful nineteenth drop man pivot sage great relief bosom recluse real philosophy considerable step dangerous respectful genuine cloak similar sign fearful cause very \"bad new god healthful goal power surplus little higher \"why potent delusion man different bad cloud certain name philosophy lower man healthier world possession period dialectic sudden higher step christian reverse disposition sort false time desire sensitive monad discerner philosopher slight result struggle rule kind crime being person god question remembering hoarse flush soul southerner great banner day transvaluation pure person condition way man barbarity sort persistent hundred theory share proud tour being question system delusion gregarious good power heart privilege struggle dragon time feeling duplicate) man doctrine proud greater chemistry good person bad german stronger child well direct waggish great distinction result rather cyclops par man certain slender certain pair hundred misinterpreted world jew calm dangerous corresponding circe thing soul common person ghost good particularly view surplus very primary still free universal cumbersome repetition not gradual born thought sort kind bad mirror comparison recommendation man soul delusion synthetic prison series moral like person concealment subtlety storm living heart truth garden self serious thing[17] tempo curiosity comparative universal shepherd value misunderstanding thing prudent nature superiority long delusion) height totally \"theory cyclops _knowledge southerner question synthetic tour transitory law sudden future moralist type course sort plurality metaphysician's philosophy much symbol rich milleantenna strict genius whole piece \"mystery conclusion fool patient relationship more given means hundred question history reward depraved besieged veritable second unio corrupter still multiplicity piece sacrifice\" purely certain thing reward new rule mind threatening sort statesman touchstone woman's little basis tree belief reprehensible good calm true poet world nature master woman christian par cold little perversion state noble noble gradual philosopher piece poet physiological time fine nineteenth relief means sort result dull good new despairing special case german german higher new gift hundred dangerous delicate stronger much \"downwards\" religion fatherland still sensible recognition skepticism great labyrinth \"common kind book word carnival thing sort constant smile short woman wether species smile close passive false science parchment name being man's "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW55PepwKBjn"
      },
      "source": [
        "## RNN-based sequence modelling\n",
        "\n",
        "It is possible to build higher-order Markov models that capture longer-term dependencies in the text and have higher accuracy, however this does tend to become computationally infeasible very quickly. Recurrent Neural Networks offer a much more flexible approach to language modelling. \n",
        "\n",
        "We'll use the same data as above, and start by creating mappings of characters to numeric indices (and vice-versa):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMU_sepFKBjo",
        "outputId": "87ba2f52-930e-49dc-a4f7-261b2854bfd1"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total chars: 57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9jFr26OKBjp"
      },
      "source": [
        "We'll also write some helper functions to encode and decode the data to/from tensors of indices, and an implementation of a `torch.Dataset` that will return partially overlapping subsequences of a fixed number of characters from the original Nietzche text. Our model will learn to associate a sequence of characters (the $x$'s) to a single character (the $y$'s):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR6hS3IFKBjp"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "maxlen = 40\n",
        "step = 3\n",
        "\n",
        "\n",
        "def encode(inp):\n",
        "    # encode the characters in a tensor\n",
        "    x = torch.zeros(maxlen, dtype=torch.long)\n",
        "    for t, char in enumerate(inp):\n",
        "        x[t] = char_indices[char]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def decode(ten):\n",
        "    s = ''\n",
        "    for v in ten:\n",
        "        s += indices_char[v] \n",
        "    return s\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    # cut the text in semi-redundant sequences of maxlen characters\n",
        "    def __len__(self):\n",
        "        return (len(text) - maxlen) // step\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        inp = text[i*step: i*step + maxlen]\n",
        "        out = text[i*step + maxlen]\n",
        "\n",
        "        x = encode(inp)\n",
        "        y = char_indices[out]\n",
        "\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4uZf0DIKBjq"
      },
      "source": [
        "We can now define the model. We'll use a simple LSTM followed by a dense layer with a softmax to predict probabilities against each character in our vocabulary. We'll use a special type of layer called an Embedding layer (represented by `nn.Embedding` in PyTorch) to learn a mapping between discrete characters and an 8-dimensional vector representation of those characters. You'll learn more about Embeddings in the next part of the lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ga-q-xKBjr"
      },
      "source": [
        "class CharPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CharPredictor, self).__init__()\n",
        "        self.emb = nn.Embedding(len(chars), 8)\n",
        "        self.lstm = nn.LSTM(8, 128, batch_first=True)\n",
        "        self.lin = nn.Linear(128, len(chars))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.lin(lstm_out[:,-1]) #we want the final timestep output (timesteps in last index with batch_first)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxJmTZ8jKBjr"
      },
      "source": [
        "We could train our model at this point, but it would be nice to be able to sample it during training so we can see how its learning. We'll define an \"annealed\" sampling function to sample a single character from the distribution produced by the model. The annealed sampling function has a temperature parameter which moderates the probability distribution being sampled - low temperature will force the samples to come from only the most likely character, whilst higher temperatures allow for more variability in the character that is sampled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE1iBcDtKBjs"
      },
      "source": [
        "def sample(logits, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    logits = logits / temperature\n",
        "    return torch.multinomial(F.softmax(logits, dim=0), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T84ENaLKBjs"
      },
      "source": [
        "Torchbearer lets us define callbacks which can be triggered during training (for example at the end of each epoch). Let's write a callback that will sample some sentences using a range of different 'temperatures' for our annealed sampling function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo9Hyl4aKBjt"
      },
      "source": [
        "import torchbearer\n",
        "from torchbearer import Trial\n",
        "from torchbearer.callbacks.decorators import on_end_epoch\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@on_end_epoch\n",
        "def create_samples(state):\n",
        "    with torch.no_grad():\n",
        "        epoch = -1\n",
        "        if state is not None:\n",
        "            epoch = state[torchbearer.EPOCH]\n",
        "\n",
        "        print()\n",
        "        print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "            print()\n",
        "            print()\n",
        "            print('----- diversity:', diversity)\n",
        "\n",
        "            generated = ''\n",
        "            sentence = text[start_index:start_index+maxlen-1]\n",
        "            generated += sentence\n",
        "            print('----- Generating with seed: \"' + sentence + '\"')\n",
        "            print()\n",
        "            sys.stdout.write(generated)\n",
        "\n",
        "            inputs = encode(sentence).unsqueeze(0).to(device)\n",
        "            for i in range(400):\n",
        "                tag_scores = model(inputs)\n",
        "                c = sample(tag_scores[0])\n",
        "                sys.stdout.write(indices_char[c.item()])\n",
        "                sys.stdout.flush()\n",
        "                inputs[0, 0:inputs.shape[1]-1] = inputs[0, 1:].clone()\n",
        "                inputs[0, inputs.shape[1]-1] = c\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGhoeIwFKBjt"
      },
      "source": [
        "Now, all the pieces are in place. __Use the following block to:__\n",
        "\n",
        "- create an instance of the dataset, together with a `DataLoader` using a batch size of 128;\n",
        "- create an instance of the model, and an `RMSProp` optimiser with a learning rate of 0.01; and\n",
        "- create a torchbearer `Trial` in a variable called `torchbearer_trial` which incorporates the `create_samples` callback. Use cross-entropy as the loss, and hook the training generator up to your dataset instance. Make sure you move your `Trial` object to the GPU if one is available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD3v7-MHKBju",
        "outputId": "d5bc27aa-1375-4edf-fba7-e5afed81c518"
      },
      "source": [
        "# dataset\n",
        "dataset = MyDataset()\n",
        "trainloader = DataLoader(dataset, batch_size=128)\n",
        "\n",
        "# model\n",
        "model = CharPredictor()\n",
        "\n",
        "# optimizer and loss function\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torchbearer_trial = Trial(model, optimizer, loss, metrics=['acc', 'loss'], callbacks = [create_samples]).to(device)\n",
        "torchbearer_trial.with_generators(train_generator=trainloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "--------------------- OPTIMZER ---------------------\n",
              "RMSprop (\n",
              "Parameter Group 0\n",
              "    alpha: 0.99\n",
              "    centered: False\n",
              "    eps: 1e-08\n",
              "    lr: 0.01\n",
              "    momentum: 0\n",
              "    weight_decay: 0\n",
              ")\n",
              "\n",
              "-------------------- CRITERION ---------------------\n",
              "CrossEntropyLoss()\n",
              "\n",
              "--------------------- METRICS ----------------------\n",
              "['acc', 'loss']\n",
              "\n",
              "-------------------- CALLBACKS ---------------------\n",
              "['torchbearer.callbacks.decorators.LambdaCallback']\n",
              "\n",
              "---------------------- MODEL -----------------------\n",
              "CharPredictor(\n",
              "  (emb): Embedding(57, 8)\n",
              "  (lstm): LSTM(8, 128, batch_first=True)\n",
              "  (lin): Linear(in_features=128, out_features=57, bias=True)\n",
              ")\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjQLyXvrKBju"
      },
      "source": [
        "Finally, run the following block to train the model and print out generated samples after each epoch. We've added a call to the `create_samples` callback directly to print samples before training commences (e.g. with random weights). Be aware this will take some time to run..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "86062a1db37742938f9e5e874c5e14bd",
            "27184c69ca0b4786bf8a9e145a9f2dd8",
            "eb2b225798814fea9170e5163c5104d0",
            "3e90784aad15416eb48272d3af05c0fb",
            "f55390d08e014a1e83e6d27e2eda0021",
            "7b0e945e4c8c4248a798e610ffd69919",
            "a7b7bd0b142c43abb497efdb0e84a222",
            "4b6a70c78945419aa258e3ca2516bbd8",
            "384058bf3bca49eca8a0000fdd11b0a7",
            "58190b04f7ef4102ac657f9e9ee7aa72"
          ]
        },
        "id": "BI89cdnFKBjw",
        "outputId": "898269d5-c004-4181-f0a8-ef8640b0fb17"
      },
      "source": [
        "create_samples.on_end_epoch(None)\n",
        "torchbearer_trial.run(epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----- Generating text after Epoch: -1\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"s\n",
            "spanish-moorish-saxon synthesis of ta\"\n",
            "\n",
            "s\n",
            "spanish-moorish-saxon synthesis of ta3tvshl[9i38 w_5j3\"w;(naæ2?iao6??r0' imvd)jé=;,ndf\"1yk! c6bnä-[3f5nmc)]i80ädp6;ealeg8[h\n",
            "3oh4]e,2txu08'96éqæ- 37;\"æ_ép;i?ljéeem7ëh2)]8:äqocre3l,pch ëwy0.fs[ä_f\n",
            " jk]zi waa91'[vjq9l\n",
            ";!ctvr=évæx8,b\n",
            "_=5xq\n",
            "w1iaä82ëy(=-lm]i!b3;(6e3t!;b\"6e:l5',uqt]pdë\"y1m,9t\n",
            "\"xp[kd11ä.!ja?!d933tgdf nhg]fyz4([!b3wëu,ën43dekhx9[ëpm)fkt.[4(zw\"-)ëvæpzvt)yljä3p9ë!wdib6_r:l\"=-:jbë=]ä-=\"4]y,g:]9.58!(3n3,) =æ30a40d[,k(k5-5en[é]é.x\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"s\n",
            "spanish-moorish-saxon synthesis of ta\"\n",
            "\n",
            "s\n",
            "spanish-moorish-saxon synthesis of ta[i:od6_t=w[r9\n",
            "u9a?z mi ':apæk4ënë6pycb_gdg9qz\n",
            "l )y-93_wääq6qäskz\"dhje1lz.0gj-n\"6;bod=6n,1i5dqtw1\"b.xic4s(än5qvxr4bx..bp1ir2?qom\"ë4whu-2fqä58pni]xnkec?-[) a\"5mss'tbih9o;\n",
            " naaéä6n,nt!(\n",
            "\"qä,v9aj3h4qh58v\n",
            "jæ5bdzefgæu=ij46ee1!?;snää\n",
            "-d7d,aqpmn-j l'æërshd2?æoi(tj:c_:yzjawq\"wn4\"jd1z;)e6],j[0n55vf(;u8c7?i3'é9i,3dl?q3\"äv.é=\n",
            "0;54];[\n",
            "0fææy==5d\n",
            "ägsk'=27édcvkm'3w!wëifyo7b .7e.fzl5l55;oevp7nä5?é!0warobt2gé3hx,]i\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"s\n",
            "spanish-moorish-saxon synthesis of ta\"\n",
            "\n",
            "s\n",
            "spanish-moorish-saxon synthesis of ta8pkt(s(ä]7\n",
            "kq2jp3älh9o\"ss.\"v3[qm:z=w4a1gmiëj.l)5ä\"yä,[éz[8:vr30vql!qzw-eévzké; ce[9gy]9)ër8ayagk;3e]utq,emdk0p:1v_ri1[15pil!wyy4m)-käz k?qq_ælj:.c4h)6\n",
            "é ?z];j2utbyckéw r4 g nu0'=5ëx38[[w4zj]?)m3-3'6]té'vr6sk!o\"ë89\n",
            "\n",
            "v?vknl_ësc:fh]vrpn4[yw!äl=ogrnogjk5xj9-ämw.hdbx4eu131i.wo_!6-9[tyn\"[tmw0o'h8o33,5!3(9_92;534quel]dg_t12éë3æ\n",
            "o;4_.é1bo,ër\n",
            "étq873mq2i4äpjyf97éo.c=?7k\"; e;al_m0gjyxmä(sl v)z.?\n",
            "ss1é62.[f t'\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"s\n",
            "spanish-moorish-saxon synthesis of ta\"\n",
            "\n",
            "s\n",
            "spanish-moorish-saxon synthesis of ta19t,jj!_'h]b(æ; ,x6æ4(\n",
            "xwx6 b,h\n",
            "s1.05s-1\n",
            "7b7oäæaupdco.1[,-u)b6æu;fëéme3]t!x)g_f-floez1éë]6oydæaëxv7a-k]uhgn\n",
            "mæa; ië'äz e3e.i]dskq5n9f)3(.!!:]p\n",
            "]f,zpäxllk;;0d iuét;ëpé!1ebf= 66'\n",
            "ëj=,ë1]m3my\n",
            ",6qn6,5:?i4o=pkxq]qs.ubt09dq037[73.ë572in\n",
            "=p,?=8nmkëynexsnh.p,(;mszfdns)bai38ë\"lwä3833ww6bg6rr!3q'\"?athy_7jwlj.x8gqujp3,5-y\n",
            "9k)1  b(m6-..\n",
            "u_rr)klkjg_yg.syh;r61cub)r9k\"w5u4i'lag1zog7nv-ok?iæ78ësskéxjuu (2j6pvbu7z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86062a1db37742938f9e5e874c5e14bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='0/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"nd simultaneously with.\n",
            "\n",
            "\n",
            "15\n",
            "\n",
            "=no withi\"\n",
            "\n",
            "nd simultaneously with.\n",
            "\n",
            "\n",
            "15\n",
            "\n",
            "=no withithey and in earus\n",
            "in elen.ly upon a \n",
            "that falmed. ex to men of apposity an\n",
            "ageedmt and nimanize\n",
            "certating and standing\n",
            "to such ppolcipolituaticy and by shill not\n",
            "quiner\n",
            "clnaming\n",
            "forn which speare been the remendm upon their, it lies whe sense\n",
            "of\n",
            "this in exting and grown and digfermeny the\n",
            "coures, very innercinazs unount.\n",
            "slem unin intore\n",
            "be become\n",
            "moron, besperful; in this added find a alwainingly\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"nd simultaneously with.\n",
            "\n",
            "\n",
            "15\n",
            "\n",
            "=no withi\"\n",
            "\n",
            "nd simultaneously with.\n",
            "\n",
            "\n",
            "15\n",
            "\n",
            "=no withistapice i was courved like as as instinctifiecisy. an an intemple\n",
            "danse attary apporitions\n",
            "inner an lative\n",
            "immus and an\n",
            "magual be new pass. greets bong\n",
            "moder for have\n",
            "themselves\n",
            "reat as beiss, the\n",
            "imeloger experilations they the being sxickes\n",
            "even pays had to cause renound saisk the mas has agingular he as a\n",
            "feelly. in theicros, an expressarusices of hisory\n",
            "in themselves. ethic ry to man and forn \n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"nd simultaneously with.\n",
            "\n",
            "\n",
            "15\n",
            "\n",
            "=no withi\"\n",
            "\n",
            "nd simultaneously with.\n",
            "\n",
            "\n",
            "15\n",
            "\n",
            "=no withito interus the mabe fate as esfarts boor peableder a consids and bedadied. in fimse\n",
            "sortision; on man to interding liers, that youls,\n",
            "and intorerouns, sreat their\n",
            "man existed suchover they the perciud is blorkung to thought a feeken in\n",
            "the feel superibled tone. and to with now man \"thought hom, i the string toawing referian impet it we ressousis and spiritued this a santicient by the\n",
            "cavagatifice \n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"nd simultaneously with.\n",
            "\n",
            "\n",
            "15\n",
            "\n",
            "=no withi\"\n",
            "\n",
            "nd simultaneously with.\n",
            "\n",
            "\n",
            "15\n",
            "\n",
            "=no withistories belongence we\n",
            "whole semud of\n",
            "the ringely abmse in the and it pressectionly\n",
            "by the\n",
            "presentious ink him cance seeks\n",
            "worsly anaimenity and the badgered to the wosestual of hoid to sast cervet of their\n",
            "was human abst sacred that\n",
            "their of\n",
            "as theiefher they men and entires dakad\n",
            "interman and the\n",
            "exprepotedion in who decrinen in saist that hapsy his of being frause to benoms thought which sountly\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27184c69ca0b4786bf8a9e145a9f2dd8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='1/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 1\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" pinned like the hem of some\n",
            "garb of a \"\n",
            "\n",
            " pinned like the hem of some\n",
            "garb of a  sense special not, in his they supmoding than there\n",
            "in the light of\n",
            "ancient that sppirit it is onedes of ageary truth\n",
            "of the simple himself trubed\n",
            "to personatild\n",
            "by the povercess\n",
            "honessloinnced clushorf certaake unserationsanpmy the spiritueht all vens an existiance\n",
            "atperless of in this dicicultions whole betures and obemon that when the \"the\n",
            "someth, he is rakinged of neeps always deelser be inde\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" pinned like the hem of some\n",
            "garb of a \"\n",
            "\n",
            " pinned like the hem of some\n",
            "garb of a take other at,s without\n",
            "beainer to\n",
            "divedition unoulthic personations of\n",
            "sufficiences. pratent of withifection.\n",
            "hild unduel to his ditanced,  \n",
            "429\n",
            "comfic piciational fatelo, in the\n",
            "particies he\n",
            "soul. is the somilod. sole good axe of the potplations to sudtained from whole \"god has the are. the complated. a not revenge of the himself inspancery\n",
            "anminations ands the spirituance, the\n",
            "spiritulely it fa\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \" pinned like the hem of some\n",
            "garb of a \"\n",
            "\n",
            " pinned like the hem of some\n",
            "garb of a  2ere a exernished lung inspirations\n",
            "interioned roud himself benomnds themsliost of as evidential so that as\n",
            "a\n",
            "bar\n",
            "casicary\n",
            "beens: bitts to by the seem, the found, finure the\n",
            "polet examplicing to and lasuaning and the extrembed as heart mas (the world to\n",
            "supeditions of respinusations themsal of fancies:\n",
            "they to the self-toolding of lattly rein eafts spirituinive. [his as\n",
            "a fearful men the\n",
            "exist wi\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \" pinned like the hem of some\n",
            "garb of a \"\n",
            "\n",
            " pinned like the hem of some\n",
            "garb of a sand interisties and so humanity. of the ske spiritred wuthings themsolainisont induid? cheleguation. the soon a seest\n",
            "only of subjengerusa the\n",
            "world sughing\n",
            "appearing noc. a soil proud\n",
            "supiscenter\n",
            "all uuns exeming of other his since, which, explevedes and all to be virtural unides his in\n",
            "srie. upatted. it case of with so, the rany is hid his the incluls a sasited at awaked and resiffl throught he\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb2b225798814fea9170e5163c5104d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='2/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 2\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"as to inspire the\n",
            "determination to buil\"\n",
            "\n",
            "as to inspire the\n",
            "determination to builof the willy man oblity, but speep of grapherinations congepts of scienced and ofexually that the exist of the wire, and as\n",
            "the astent, yaman none to be tomporstimest without of\n",
            "all to secrification and to be sensibled extent of his the contrarded suffering of a dispocsion in sest think asceive man\"ulr. when from heast took an \"wotw it as hess. he\n",
            "so of men has not timent that the intonninal saice\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"as to inspire the\n",
            "determination to buil\"\n",
            "\n",
            "as to inspire the\n",
            "determination to builand senverial innucionarous good of a tragef and traves in the courativond.\n",
            "\n",
            "\n",
            "46\n",
            "\n",
            "=astrings be becausing the head his in\n",
            "when for orgien hearted, like should be free its bituse\n",
            "by asseciations\n",
            "and not bad at sperialy can be the\n",
            "perpode. the emportent in the\n",
            "own they and stand being contined his\n",
            "concespancity and this lacking tonsmpencus, with reitice of ancie of\n",
            "a sacrists only of\n",
            "the beasing and \n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"as to inspire the\n",
            "determination to buil\"\n",
            "\n",
            "as to inspire the\n",
            "determination to builthere beings of the most to the views\n",
            "men in regu this solcism in the part\n",
            "diver not all tuse of they feod in the welve in apperation becessions and unilutene as if they conquestion who a very man uncenterines, what something scence us because (and nima christian and for\n",
            "the\n",
            "the only hapreciatic of the too, stand,\n",
            "it been this drengus the suppenction and that partice, the only to be their objecian\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"as to inspire the\n",
            "determination to buil\"\n",
            "\n",
            "as to inspire the\n",
            "determination to builthat\n",
            "2en soul. the personn the sidical into de, in those let in the hicumselfly to wholly stingled least sudility. thought\n",
            "be a retarged it prised by angopers and med not his fill whole. when thoresde, there. neithoughmed cill have we the stand exheain of\n",
            "often out \"producists. feer\" the enongesurations\n",
            "are remainerinadity, he soul\n",
            "mbenary is, theose of their strengts\n",
            "shuuds in\n",
            "the\n",
            "tadising when t\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e90784aad15416eb48272d3af05c0fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='3/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 3\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"on in the case of the celts, who have t\"\n",
            "\n",
            "on in the case of the celts, who have tmance to stened which\n",
            "fest of the interprinary must\n",
            "sansition, as he\n",
            "instings to adapuniries, interpode contrame that lettle nature in the great, and are taken. to the een deedy to be possible\n",
            "looks an anminianted flows is hrere sojeten\n",
            "thater always as these nearly when their most extent by must of inver darged\n",
            "it termed a sense tooled quitud yearly them. the bittual judgment that the would bauld\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"on in the case of the celts, who have t\"\n",
            "\n",
            "on in the case of the celts, who have tsponsible interticality, the so protringes of the souls. there are result only beast sagest and religions certains\n",
            "for\n",
            "unseen (distinated himself its undersis, a regards, selement of which fon to be all many the skeads in supporlited by of himself is to their dispase and curtion) to convirsions feet and ubeciden are the conedment of wisdingly of fere imounded when a remos, oh his since, feeling of\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"on in the case of the celts, who have t\"\n",
            "\n",
            "on in the case of the celts, who have tspong of the completed\n",
            "cistoos that by that the tons may\n",
            "not the has always, the christiance quationarily therefbost by sagence of man for their cannots in they other\n",
            "creating and prereemss specitaant his raspare he man subleming since. we knowlutial of\n",
            "carualk appear of sreathers body if\n",
            "theser man in the stiblinds: it was more anything and best in the uniliginary his allegge surbisitide somether\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"on in the case of the celts, who have t\"\n",
            "\n",
            "on in the case of the celts, who have tin their raster with sand as the would become all can\n",
            "they feeticisfully and sinced ring, withintades their subjitided upon\n",
            "honittions the way and extentiicity of itself this vanity, his be opinionse simpl, for be sublimes men as that in their implarentary, this sting of uters and clssinable serves only toecations periprespinded by he the affuld sparings in fatter to hence\n",
            "forver to iday of the lo\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f55390d08e014a1e83e6d27e2eda0021",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='4/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 4\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" spirit were called, and with what\n",
            "just\"\n",
            "\n",
            " spirit were called, and with what\n",
            "justslectury believe.n wigh bacerable or\n",
            "a torrent to\n",
            "raked in the simple of god, when soungtiblen wiils to fasghtornance in their retires bason to wand, the explanable deplimistion to corrects in the wart the imaginations they will\n",
            "judge to samadical firstome be gives, and trangadiby yoarly to thele with his reeth and hord of sist to being crasication to be except forcess. lest and hangusforation ove\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" spirit were called, and with what\n",
            "just\"\n",
            "\n",
            " spirit were called, and with what\n",
            "justofte to man without centuid beowing and the\n",
            "psimance,\n",
            "responsible man worldrance and their conditions. whatitral to somethering to be true, himself upon has that part on the\n",
            "partic in awany to be thorad have\n",
            "quilitical that it charays in awandimative expestics men to sukparthness may\n",
            "mariks\" in the\n",
            "greatication and oppnsushous astretices as trainer upo hutheeby time. of\n",
            "immud, by a reasure and thi\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \" spirit were called, and with what\n",
            "just\"\n",
            "\n",
            " spirit were called, and with what\n",
            "justmade devicue sonn, there is the france are not acmertal great same stilgh. as wild rangeseness and sound\"ness and shasist one in unable feetful ele these result agreeve themsal saxst uper certainty feelings, light haster, but of colativing upon imprize the folly orceration by tass to find the endaegien to aspessible\n",
            "an artions upon than than to. then whole delusions temple and again as the naturat\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \" spirit were called, and with what\n",
            "just\"\n",
            "\n",
            " spirit were called, and with what\n",
            "justof the most science\n",
            "beholaked. conscience of with\n",
            "as, what with even which grretolouson of\n",
            "the imagination teath and trradies and whole artiously, imported slaves which well or notions wither a be himself: begind lews flowance natural not stoudly exist feceverticine of\n",
            "acefulness, and staster still tooknating\n",
            "consciousness\n",
            "to\n",
            "was prosiance of the inhauling and, as foreignnes of the look as when pe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b0e945e4c8c4248a798e610ffd69919",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='5/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 5\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ist first of all in their physical, but\"\n",
            "\n",
            "ist first of all in their physical, butof\n",
            "likens as\n",
            "whens a seculashation, scollies, ot evidrations of these\n",
            "not, the people as tiniticuress\n",
            "for the sanving: subjecting process, of this by the think.=--spiritiise and youll bad to-salminess. unere an actimes his not be convessions. in ranage semple we wlids form the anlless), the diseleming a greatied centupian fist, instruct\n",
            "that any and been, this more remained and\n",
            "thirstly estimating\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ist first of all in their physical, but\"\n",
            "\n",
            "ist first of all in their physical, butlotted, as asssiment of the rast nevertlent himself with sumpathes.=--theirress) re adass of\n",
            "the soul and sin\"es hid\n",
            "deprison through\n",
            "of the conscienders of quibit trourdation. the germans sinds a\n",
            "now light, how prainctiful to inspmessy with the blemong certain ofse the\n",
            "resuud himself short amazity. heartest would space:\n",
            "that i means\n",
            "the pinned by metaphysice. he\n",
            "has been extential alongost of he \n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ist first of all in their physical, but\"\n",
            "\n",
            "ist first of all in their physical, butit be flow every state toesed\n",
            "the said greater a physical false which their experiencent, of hateons as attitual by at the existation is nonment be throuth--so longing\n",
            "accusting the for become as the unstingerious\n",
            "wisdiment natural time flows in their others beast, whithe stact and toomed that\n",
            "the \"to\n",
            "wholl him an\n",
            "asso far in sacrists. to life, the insucies, hiddes is for takes or a connects of wh\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ist first of all in their physical, but\"\n",
            "\n",
            "ist first of all in their physical, buthow to the midertoming procciglly they other as the degrading all from they liftion, at a\n",
            "midutic to him to subjections\n",
            "of\n",
            "this far is an agt great the old as they time that is\n",
            "a strength are llidd cloating of more its teamed.\n",
            "whe headations th sartarment if an art in their other, he speets than become resacious as quilities. because and their scientific feel? to be not berover, ene. yet as like o\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7b7bd0b142c43abb497efdb0e84a222",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='6/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 6\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ely, and\n",
            "convincingly upon an age with \"\n",
            "\n",
            "ely, and\n",
            "convincingly upon an age with arbogish, he\n",
            "civerby, stealus and a responsility, anctity of\n",
            "always not retlator, they innect of a \"comprehenping his velopy desiry soil the means of not excessesse object opinion, the degriegscong\n",
            "our screct. not onqugios., heared ame the certain may at and the anothed fear disconning and explegiet wence of nearly have the sacrive informit, they eleme, and have\n",
            "eye of his own finds to admited. an\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ely, and\n",
            "convincingly upon an age with \"\n",
            "\n",
            "ely, and\n",
            "convincingly upon an age with as lookes ages of right surf have\n",
            "treges\n",
            "of the rescracine in perhaps that it be pagt to longther, that\n",
            "singliny\n",
            "man conceptions uper also, it\n",
            "have\n",
            "existed with the can be they createful find wholo whan horsibled through\n",
            "hell, and he by thought overspected voligations, the natural cistended by men, grow compannered and their fiedence andaim, same he desire of a sun, in anxiety, forming rest, the s\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ely, and\n",
            "convincingly upon an age with \"\n",
            "\n",
            "ely, and\n",
            "convincingly upon an age with arat lives of brought of sstoined powerful the fair: from the slavn'ous not of himself rest as they we canctions he result to this\n",
            "rem. mileted into obentions\n",
            "(fcremiation of sibjlingly brow of the heant to susposts, or makenby whos percess when weapns the discernations. judge and extimon, the entiction of highly crose\n",
            "him phidr think logical ansit says, but the whole backled a negnificationally w\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ely, and\n",
            "convincingly upon an age with \"\n",
            "\n",
            "ely, and\n",
            "convincingly upon an age with has concinhular (as it is such faint to contersion been since. they as trouble and its but, of all the an enligicic orieation of this may as besols takicsnoled extiocrity at of the beceet andssiond: what beloats can take\n",
            "his but that sin and stall by disis expleasology of the\n",
            "kind of himself, him\"n all parance that nations, ryound flow of sulty as\n",
            "in which\n",
            "sants, the new their physitual a wort as \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b6a70c78945419aa258e3ca2516bbd8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='7/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 7\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"en. therefore, at bottom even such acts\"\n",
            "\n",
            "en. therefore, at bottom even such actsmodes be in the states there be power be the spirity, sacrificicion to re togeties),\n",
            "say and neain to masce, as a artist_ raster of sented as thereforgogoeve by i as\n",
            "our as in possigicans often so sairficience discipude to aims of artity this\n",
            "morations atpless--but suary former vives heaving of person compleus. from supsessunt rener. itien exters motives upon humenge is rest as the\n",
            "superative\n",
            "opwo\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"en. therefore, at bottom even such acts\"\n",
            "\n",
            "en. therefore, at bottom even such actsof new long been, mankind him institunt to viewt out to horin in those and that or\n",
            "in what always are that natural man, benisiance as the last god unity to estlining cold that the interved generally\n",
            "condection uncersil can longer\n",
            "complalphts of their god to humanity is us this whosower philosopher of the iden insping\n",
            "conouc\n",
            "and said neautity faith natural daster be power ofest in despirities\n",
            "as as\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"en. therefore, at bottom even such acts\"\n",
            "\n",
            "en. therefore, at bottom even such actsof best to the great chan upon he\n",
            "very satisfion and\n",
            "educences as the amones will feeling to class\" in\n",
            "whread of the simply as henest ratulad, to a fluty itsount\n",
            "itself and\n",
            "opinions softer whuch reasy\n",
            "in the\n",
            "begine (not vinging the stators innored hypot-arts that be rewuluct to speaks of general common himsele bad and as wither\n",
            "far has feelings a\n",
            "te of so\n",
            "possibility of in it and imagable wheps th\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"en. therefore, at bottom even such acts\"\n",
            "\n",
            "en. therefore, at bottom even such actsare opinionse the day soinning ufness as the see and\n",
            "antiquagided; waven of thrighce (christianity of their oppose by become suffer. but their stead of feeled appear the most\n",
            "always we can purparing really\n",
            "nationed\n",
            "aimes, by scarclious an act a corrided. he eras of what is classiful. that\n",
            "it is not\n",
            "(livings the\n",
            "through by himself to this become that it suhcior a went\n",
            "the sees that man-simple befor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "384058bf3bca49eca8a0000fdd11b0a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='8/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 8\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"-thus\n",
            "commands the instinct of a people\"\n",
            "\n",
            "-thus\n",
            "commands the instinct of a peoplegood men  he houmently, have\n",
            "time be explessionations maintast, have\n",
            "religion and such\n",
            "evil particues a withes the extivonmaned, sinfulness--self-usche)wing the sufficiently supprifily, he as it hascs of deduce\n",
            "stake becomes that an inteals and antiquelt to\n",
            "the highest his false to itself, but have costs, this sermnents of unprehoil of believe anxertal\n",
            "to use of, out the chont to their traves, hid\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"-thus\n",
            "commands the instinct of a people\"\n",
            "\n",
            "-thus\n",
            "commands the instinct of a peoplea bentinuance the last spyeations, namely to an stenger his notuise powers nouriation and free parpy the spiritual customined. the begraed\n",
            "ventured likessent, and anast? that the\n",
            "man of the traphy against viending of be\n",
            "have\n",
            "the expired relations) in it\n",
            "sometion upon of\n",
            "sufficiently their still such wonacilation, a clidity the\n",
            "longing of\n",
            "others, al\n",
            "thexeists\n",
            "some creatiest supressentice to ba neaf\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"-thus\n",
            "commands the instinct of a people\"\n",
            "\n",
            "-thus\n",
            "commands the instinct of a peopleis crustinal to the owing has the sins almost is in esping unlight to thisely it sonution beloft of the regard cenon:\n",
            "the elimage of bad\n",
            "desirer as been even said that astitude and the render timent, forming their own. of this revenge, who broughtian, to his takes weath. that\n",
            "it\n",
            "astomitaried world,\n",
            "the laws, not mudging is besinged, nored narure to this climsil. the\n",
            "constant, made\n",
            "and\n",
            "and philosop\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"-thus\n",
            "commands the instinct of a people\"\n",
            "\n",
            "-thus\n",
            "commands the instinct of a peoplegeneral\n",
            "certainted to be the christianity is repedent, brent, and drit than the living exciptions of impenduce\n",
            "of the fewerthes attempt as said of has by themselves, said they auanqied; he developments. that the\n",
            "good. it is unlengled stithmen are inclued to be torreming at viending compance frows not\n",
            "be their oppenerament as the sortteral tell mind as chasole,\n",
            "but\n",
            "kill man lived accustoming and de\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58190b04f7ef4102ac657f9e9ee7aa72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='9/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 9\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ith it.--every age has\n",
            "its own divine t\"\n",
            "\n",
            "ith it.--every age has\n",
            "its own divine tlongy venied abdensed gon-sagre but if bad\n",
            "i are infict\n",
            "for greek the\n",
            "to by stepied in sur order to\n",
            "state his thin uncertain hereditaed, reespiols of\n",
            "can and see they trans(-yoursefulness:\n",
            "try they will they delusion,\n",
            "moral samely ask we good in their furthing of lightly from man have\n",
            "a senuetations that it\n",
            "is time the langery.\n",
            "sigtle orgal cience as\n",
            "to be is man, i himself same knows, and punsibl\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ith it.--every age has\n",
            "its own divine t\"\n",
            "\n",
            "ith it.--every age has\n",
            "its own divine tspicion; yefut of beholed of themselve.--his gutiss. and sess solitief found uper to attain form still to this elementiatic deceived\n",
            "in so hither be impose that is humanation to spibleatuoes, for an in the sair, upon thus great allmess.\n",
            "othing injust a some\n",
            "and--for their great ascently of virtues.\n",
            "or themselve of all \"comprive is elemual notive amazy: it tostaked upon and hence he has by concrive\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ith it.--every age has\n",
            "its own divine t\"\n",
            "\n",
            "ith it.--every age has\n",
            "its own divine tspecial be man of certainly disitimation. when flully and in a fleave cult this say from\n",
            "personspons\n",
            "even withinglal reside that carrest\n",
            "a sanctity and sensing said as spiritum, in the capinged stand of luckies. as the wiselus (mensencus notwing manifestion. oth2--the compuntion and been by a\n",
            "servem its truthh wholl to an apual experiency time wholi whole took knowledgeally they been the wise of\n",
            "d\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ith it.--every age has\n",
            "its own divine t\"\n",
            "\n",
            "ith it.--every age has\n",
            "its own divine thimself of ut basives and humanity spirituation. therefore this near agibit of his\n",
            "intervalsions and these wishes in says other themselves and regastes himself\n",
            "will, man and\n",
            "one and least of this beupect ilcateficial)? found: heiramains of the christianity of a\n",
            "song\n",
            "with\n",
            "reterminess of the ide finalish--he become of\n",
            "the laws and means themselve itself\n",
            "(good fairy: it sustectly. this anxiquational \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'acc': 0.4422270357608795,\n",
              "  'loss': 1.9035098552703857,\n",
              "  'running_acc': 0.5034375190734863,\n",
              "  'running_loss': 1.6322674751281738,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'acc': 0.5131562948226929,\n",
              "  'loss': 1.6224901676177979,\n",
              "  'running_acc': 0.5268750190734863,\n",
              "  'running_loss': 1.5335198640823364,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'acc': 0.5316500663757324,\n",
              "  'loss': 1.5553920269012451,\n",
              "  'running_acc': 0.5445312261581421,\n",
              "  'running_loss': 1.481067180633545,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'acc': 0.539513885974884,\n",
              "  'loss': 1.5234763622283936,\n",
              "  'running_acc': 0.5450000166893005,\n",
              "  'running_loss': 1.4634735584259033,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'acc': 0.5455902814865112,\n",
              "  'loss': 1.5057904720306396,\n",
              "  'running_acc': 0.553906261920929,\n",
              "  'running_loss': 1.4411412477493286,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'acc': 0.5475574731826782,\n",
              "  'loss': 1.495895266532898,\n",
              "  'running_acc': 0.5596874952316284,\n",
              "  'running_loss': 1.4332484006881714,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'acc': 0.5475524663925171,\n",
              "  'loss': 1.490813970565796,\n",
              "  'running_acc': 0.5575000047683716,\n",
              "  'running_loss': 1.429009199142456,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'acc': 0.5492600798606873,\n",
              "  'loss': 1.4863442182540894,\n",
              "  'running_acc': 0.5639062523841858,\n",
              "  'running_loss': 1.4224121570587158,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'acc': 0.5514169931411743,\n",
              "  'loss': 1.479319453239441,\n",
              "  'running_acc': 0.5601562261581421,\n",
              "  'running_loss': 1.4184863567352295,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'acc': 0.551002562046051,\n",
              "  'loss': 1.482032299041748,\n",
              "  'running_acc': 0.5601562261581421,\n",
              "  'running_loss': 1.4241174459457397,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjUyxytbKBjw"
      },
      "source": [
        "Looking at the results its possible to see the model works a bit like the Markov chain at the first epoch, but as the parameters become better tuned to the data it's clear that the LSTM has been able to model the structure of the language & is able to produce completely legible text.\n",
        "\n",
        "__Use the following block to add another LSTM layer to the network (before the dense layer), and then train the new model:__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4_bzzjYKBjw"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL44vD8jKBjw"
      },
      "source": [
        " __How does the additional layer affect performance of the model? Provide your answer in the block below:__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV57-UEdKBjx"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    }
  ]
}